        教你使用数据挖掘比赛大杀器Xgboost
		
1、视频地址 
   https://v.qq.com/x/page/r0549mlse11.html
   
2、视频笔记 
    xgboost是对n棵决策树进行集成的结果
	1.0 决策树
	    a、树形结构，每个内部节点表示一个属性上的测试，每个分枝代表一个测试的输出，每个叶子
		    节点代表一种类别
	     
		b、归纳学习，决策树学习是以实例为基础的归纳学习
		
		c、决策树学习采用的是自顶向下的梯度下降方法，其基本思想是以信息熵为度量，构造一颗熵值
		   下降最快的树，到叶子节点处的熵值为零，此时每个叶子节点中的实例都属于同一类
		
		Eg:
		  Input：M个样本，每个数据包括年龄、性别、职业、每日使用计算机时间等
          Output:该样本是否喜欢打游戏
		
		DT特点：
		   1、学习过程不需要了解背景知识，是需要对训练实例进行较好的标注，就能够进行学习
		   2、监督学习
		   3、从一类无序、无规则的事物中推理出决策树表示分类规则
		   
	  **建立决策树的关键：在当前状态下选择那个属性作为分类依据。根据不同的目标函数建立的决策树
		主要有以下三种算法：
		   1、ID3:信息增益
		   2、C4.5:信息增益比
		   3、CART：Gnni系数
		区别：以上三种算法结构一样，但是目标函数的构建不一样
		
	1.1 随机森林：
		a、集成学习
           算法基础也是个决策树
        b、构建过程，分为以下四个步骤：
           1、从样本集中用Bootstrap采样选出n个样本
		   2、从所有属性中随机选择k个属性，选择最佳分割属性作为节点建立CART决策树
		   3、重复以上两步m次，即建立个m棵树CART决策树
		   4、这m个CART形成随机森林，通过投票表决结果，决定数据属于那一类
		   
		特点：
		   可以并行计算
		   投票选择，也可以看做并行
		   
		 
	2 提升(Boosting)  GBDT   
	  随机森林算法框架是Adaboost，GBDT使用的是boosting的算法框架。
	    a、提升是一个机器学习技术，可以用于分类和回归问题，它每一步都产生一个弱预测模型
	      (如决策树)，并加权累加到总模型中；如果每一步的若预测模型生成都依据损失函数的梯度
		  方向，则称为梯度提升(GB)。
        b、梯度提升算法首先给定一个目标损失函数，它的定义域是所有可行的若函数集(基函数)，
		  提升算法通过迭代选择目标函数一个负梯度方向，来逐渐逼近局部极小值 
		c、提升的理论意义：如果一个问题存在弱分类器，则可以通过提升的方法得到强分类器
		
	树形算法对数据量不太敏感，有比较强的表达能力，故在训练时要调整模型参数防止过拟合，主要包括：
	参数正则化：
		1、对训练集拟合过高会降低模型的泛化能力，需要用正则化技术来降低过拟合
		2、树的层数设置： 4 =< layer <= 8
		3、叶节点包含的最少样本数目，防止出现过小的叶节点，降低预测方差
		   不能太大，也不要太小
		4、梯度提升迭代次数M可以降低训练集的损失值，但会有过拟合风险
		
	GBDT小节：
		1、损失函数是最小平方误差，绝对值误差等为回归问题，误差函数换成多类别
		   logistic似然函数则为分类问题
		   
		2、对目标函数分解为若干基函数的加权和
	
    GBDT 和 Xgboost区别：
	    1、目标函数
		   相比于原始的GBDT，Xgboost的目标函数加了正则项，使得不容易过拟合，正则项用的是L2范式
		   
		2、Xgboost对打分函数做了改进
		   Xgboost使用了一个二阶的目标函数展开式，对目标函数进行了一个近似。
		   

    3. Xgboost简介：
		3.1 Xgboost原理？
			1、相对于传统的GBDT，Xgboost使用了二阶信息，可以更快的在训离集上收敛
			2、由于'随机森林族'本身具备过拟合的优势，因此，Xgboost一定程度上也具有有该特性
			3、具有更快的训练速度。Xgboost使用了C/C++语言来实现，同时具备并行/多核计算
			   Xgboost在进行并行计算是，在叶子节点计算信息增益时，进行的并行化
			   随机森林并行计算时，在每棵树的粒度上进行的
		3.2 与传统的GBDT有那些改进？
			Xgboost小节：
			1、相对于传统的GBDT，Xgboost使用了二阶信息，可以更快的在训离集上收敛
			2、由于'随机森林族'本身具备过拟合的优势，因此，Xgboost一定程度上也具有有该特性
			3、具有更快的训练速度。Xgboost使用了C/C++语言来实现，同时具备并行/多核计算
			   Xgboost在进行并行计算是，在叶子节点计算信息增益时，进行的并行化
			   随机森林并行计算时，在每棵树的粒度上进行的
		3.3 实例实验
	
3、阐述RF adaboost gbdt xgboost？（面试问题）
   3.1 RF
       RF是Bagging的典型代表。
	   
   3.2 Adaboost
       AdaBoost算法是Boosting算法族的典型代表，它基于“残差逼近”的思路，采用“重赋权法”，即是根据每个基学习器的结果调整样本权重，
	生成新的更加关注于错误样本的数据分布，然后在新数据分布上继续以损失函数最小为目标训练新的基学习器。同时根据这些基学习器
	的训练误差，对基学习器赋权，最后采用加权求和得出集成模型。

	AdaBoost旨在减小学习的偏差，能够基于泛化精度很差的学习器个体构建出强集成
	训练过程中，每个新的模型都会基于前一个模型的表现结果进行调整，这也就是为什么AdaBoost是自适应（adaptive）的原因，即AdaBoost可以自动适应每个基学习器的准确率。。
   
   3.3 GBDT
	  GB（GradientBoosting）可以被看作是AdaBoost的变体，最大不同之处在于GB在迭代优化过程中采用了梯度计算而非加权计算。
	GB通过在每一步残差减少的梯度方向上训练新的基学习器，最后通过集成得到强学习器。
	   
	   对于回归问题的提升树算法来说，只需简单地拟合当前模型的残差。即每一轮产生的残差作为下一轮回归树的输入，下一轮的回归树
	的目的就是尽可能的拟合这个输入残差
	
   3.4 Xgboost 
	  
	  
  ***GBDT与Adboost的区别
	  GBDT与Adboost最主要的区别在于两者如何识别模型的问题。Adaboost用错分数据点来识别问题，通过调整错分数据点的权重来改进模型。
	  GBDT通过负梯度来识别问题，通过计算负梯度来改进模型。  
	  
  ***GBDT与Adboost的区别
		GBDT算法只利用了一阶的导数信息，xgboost对损失函数做了二阶的泰勒展开，并在目标函数之外加入了正则项对整体求最优解，
		用以权衡目标函数的下降和模型的复杂程度，避免过拟合。所以不考虑细节方面，两者最大的不同就是目标函数的定义，接下来
		就着重从xgboost的目标函数定义上来进行介绍
		
		
		
		
		
4、使用xgboost/gbdt的时候,调节树的深度为6左右就可以达到很好的效果 但是使用DT或者RF的
   时候深度要到15或者以上才可以，为啥？
   
   偏差和方差   <GBDT工作过程实例：学习的是残差。Adaboost：分类错误的样本给更高的权重>
   bagging和boosting，笼统的说就是建树的机制不一样

   答： 1、随机深林和GBDT都属于集成学习(ensemble learning)的范畴，集成学习下有两个重要的策略，
         即Boosting和Bagging。
   
        2、Bagging做法：
	      每个分类器都随机从原样本中做有放回的采样，然后分别在这些采样后的样本上训练分类器         
		  然后，再把这些分类器组合起来。简单的多数投票决定最后结果。
		  其代表算法随机森林(RF)
		  
		  Boosting做法：
		  他通过迭代地训练一系列的分类器，每个分类器采用的样本分布都和上一轮的学习结果有关。
		  其代表算法是Adaboost、GBDT
		  
		3、就ML算法而言，其泛化误差可以分解为两部分，偏差(bias)和方差(vatiance)
		   偏差指的是算法的期望预测和真实预测之间的偏差程度，反映了模型本身的拟合能力
		   方差度量了同等大小的训练集的变动导致学习性能的变化，刻画了数据扰动所导致的影响
		   
		   模型越复杂时，拟合的程度就越高，模型的训练偏差就越小。但是此时如果换一组数据
		   模型的变化就会很大，即模型的方差就很大。所以模型过于复杂的时候导致过拟合
		   
		   当模型越简单时，即使我们再换一组数据，最后得出的学习器和之前的学习器的差别就不那么大，
		   模型的方差很小。但是因为模型简单，所以偏差会很大。
		   
		   Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；
           bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效果更为明显
	   
           也就是说，当我们训练模型时，偏差和方差都得照顾到，漏掉一个都不行
		   
		      对于Bagging算法来说，由于我们会并行地训练很多不同的分类器的目的就是降低这个方差(variance)
		   ,因此采用相互独立的基分类器多了以后，h的值自然就会靠近。所以对于每个基分类器来说，目标就是
		   如何降低这个偏差(bais),所以我们会采用深度很深甚至不剪枝的决策树。
		   
		      对于Boosting来说，每一步我们都会在上一轮的基础上更加拟合原数据，所以可以保证偏差(bais),
		    所以对于每个分类器来说，问题就是如何如何选择variance更小的分类器，即更简单的分类器，所以
			我们选择了深度很浅的决策树。
		    
5、调参
    Bagging：
      RF：<调参对随机森林来说，不会发生很大的波动,故常常用默认参数也能达到很好的效果>
	 1、（树的个数）调整过程类影响参数；
	     （自助采样--有放回采样，重复m次得到m个样本）对样本进行子采样来降低子模型之间的关联度，从而降低整体模型的方差。
     2、树的深度一般设置为15左右
	 3、A. max_features：随机森林允许单个决策树使用特征的最大数量。增加max_features一般能提高模型的性能，但会降低算法的速度
	    B. n_estimators：建立子树的数量（基分类器）。较多的子树可以让模型有更好的性能，但同时让你的代码变慢
	    C. min_sample_leaf：最小样本叶片大小。较小的叶子使模型更容易捕捉训练数据中的噪声,
		                 我更偏向于将最小叶子节点数目设置为大于50。
	Boosting：
	gbdt重要参数：
		（1）损失函数（分类：指数损失函数，对数损失函数；回归，平方误差等）
		（2）调整过程类影响参数（比如树的个数，学习率）； 
		（3）调整子模型类影响参数（比如树的深度，叶节点最小样本数） deep=6左右 
       过程类参数主要是影响偏差，子模型类参数主要影响方差
	 
	xgboost重要参数：<Xgboost模型调参：https://www.2cto.com/kf/201607/528771.html>
	XGBoost的作者把所有的参数分成了三类：
		1、通用参数：宏观函数控制。
		2、Booster参数：控制每一步的booster(tree/regression)。
		3、学习目标参数：控制训练目标的表现。
		
	 a. 选择较高的学习速率(learning rate)。
	   一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动
	 b、选择对应于此学习速率的理想决策树数量。
	    XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。
	 c、对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)
	 d、 xgboost的正则化参数的调优。(lambda, alpha)
	     这些参数可以降低模型的复杂度，从而提高模型的表现。
	 e、降低学习速率，确定理想参数。
	 
	 举例： <先给一个初始值参数，然后，通过cv进行参数调优>
	 xgb4 = XGBClassifier(
			 learning_rate =0.01,   #学习速率
			 n_estimators=5000,     #建立子树的数量，数量越大，模型有更好的性能。但也越慢
			 max_depth=4,           # 树深度
			 min_child_weight=6,    # 
			 gamma=0,               #超参，
			 subsample=0.8,         #这个是最常见的初始值了。典型值的范围在0.5-0.9之间
			 colsample_bytree=0.8,  #这个是最常见的初始值了。典型值的范围在0.5-0.9之间
			 reg_alpha=0.005,
			 objective= 'binary:logistic',
			 nthread=4,	
			 scale_pos_weight=1,
			 seed=27)
	modelfit(xgb4, train, predictors)
	 
	 
6、所以boosting算法中有以下几个问题需要解决：
	如何训练弱学习器
	如何计算学习误差率e
	如何更新弱学习器权重系数α
	如何更新样本权重D
	使用哪一种集成策略
	只要是boosting大家族的算法，都要解决以上5个问题	