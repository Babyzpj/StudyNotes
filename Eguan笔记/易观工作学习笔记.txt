pandas读取csv文件遇到问题

1、下面读法默认第0行是表头，则
   df = pd.read_csv('filename')
   
2、如果想第1行做表头，则
2、如果想第1行做表头，则
   df = pd.read_csv('filename'，header=1)
   
3、如果第0行的表头，但第一行的列数，和下面数据的列数不相等，
   则数据默认右边对其，自动闪出下面的n列数据作为所索引
   eg:第0行有8列
      下面所有数据有10列，则pd.read_csv读取出来时，
	  第0行会的8列数据，会自动对齐的下面数据的后8列
	  留出剩余数据的前2列。

4、用pandas的get_dummies([],axis=1)进行one-hot特征转化
   pandas处理中少量的数据性能还不错，但是当处理大量数据时，性能就不行了。
   pandas处理百万行，在原数据基础上使用get_dummies()编码为几万列的数据的时候，要么内存不足，
   要么直接死机.
   
4.1 one hot编码的优点： 
	1.能够处理非连续型数值特征。 
	2.在一定程度上也扩充了特征。比如性别本身是一个特征，经过one hot编码以后，就变成了男或女两个特征。
	注意：此处一定将训练特征和测试特征一起转化，因为转化之后数组的维度将会发生变化，有一个不转化，就会出错。  
   
5、对于缺失数据一般处理方法为滤掉或者填充。 
   对于一个Series或DataFrame，dropna()函数同样会丢掉所有含有空元素的数据
   dropna()函数返回一个包含非空数据和索引值的Series
      df = pd.read_csv('/./file.csv')
      df.dropna()                       # 参考http://www.cnblogs.com/sirkevin/p/5767532.html
   
6、. 导出到csv文件
   dataframe可以使用to_csv方法方便地导出到csv文件中，如果数据中含有中文，一般encoding指定为”utf-8″,
   否则导出时程序会因为不能识别相应的字符串而抛出异常，index指定为False表示不用导出dataframe的index数据。
      df.to_csv(file_path, encoding='utf-8', index=False)
      df.to_csv(file_path, index=False)

7、pandas（dataFrame或series）和sklearn(list或array)的数据格式是不一样的,将其转化方法如下：
     首先使用np.array()函数把DataFrame转化为np.ndarray()，再利用tolist()函数把np.ndarray()转为list。
        # -*- coding:utf-8-*-
		import numpy as np
		import pandas as pd

		data_x = pd.read_csv("E:/Tianchi/result/features.csv",usecols=[2,3,4])#pd.dataframe
		data_y =  pd.read_csv("E:/Tianchi/result/features.csv",usecols=[5])   #pd.dataframe

		train_data = np.array(data_x)      # DataFrame格式转化为np.ndarray()格式
		train_x_list=train_data.tolist()   #list
		print(train_x_list)
		print(type(train_x_list))

8、 将array转化转化为DataFrame类型
		# -*- coding:utf-8-*-		
		import pandas as pd
		import numpy as np

		s1=np.array([1,2,3,4])
		s2=np.array([5,6,7,8])
		df=pd.DataFrame([s1,s2])    # 如何DataFrame()里面有一个参数，就去掉[]
		print df

		
9、 归一化的目的只是训练的时候会更快的收敛。由于维度太大，如果不采用归一化处理的话，
     各个点间的距离值将非常大，故模型对于待预测点的预测结果值都判为同一个值。
		# 数据归一化
		from sklearn import preprocessing
		min_max_scaler = preprocessing.MinMaxScaler()
		
		#X_train_minmax都是array()类型，fearuure23可以是array也可以dataFrame类型
		X_train_minmax = min_max_scaler.fit_transform(feature23)   #将feature23作为一个整体进行归一化                             
		X_train_minmax                   # array类型
		
10、 F1值是精确率和召回率的调和均值
	是统计学中用来衡量二分类模型精确度的一种指标。它同时兼顾了分类模型的准确率和召回率
	
11、Centos下查看某个文件夹大小命令：
      du -sm file/   或
	  du -sh file/
   
    查看当前所在文件夹大小步骤：
	   pwd             # 得到当前所在路径(文件夹)为 /**/*
	   du -sh /**/*    # 查看所在文件夹大小

12、评估指标ROC曲线
    1、ROC曲线和AUC常被用来评价一个二值分类器（binary classifier）的优劣。
    2、ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。
    3、ROC曲线越靠近左上角,试验的准确性就越高。
    4、最靠近左上角的ROC曲线的点是错误最少的最好阈值，其假阳性和假阴性的总数最少。
	
	
13、AUC
    ROC曲线，中文翻译是受试者工作特征曲线。如果横轴是1-特异度，纵轴是灵敏度。那么就会形成1个弯曲的曲线。
	这个曲线和45度的直线会形成一个曲线下面积(area under ROC)，简称AUC。AUC越大，说明判断的效果越好。
    
14、SVD

15、SGDClassifier和SGDRegressor可以用于极大的数据集。然而，如果数据集过大的话，最好从数据中取样，
    然后和小数据一样分析建模，未必一开始就要在整个数据集上跑算法。
	
16、https://www.zhihu.com/question/52992079   
     16.1  在实践中，特征工程、调整算法参数这两个步骤常常往复进行。
	 16.2  线性分类器中，最好用的是LogisticRegression和相应的LogisticRegressionCV
	 16.3  sklearn中最常用的ensemble算法是RandomForest和GradientBoosting。
	       不过，在sklearn之外还有更优秀的gradient boosting算法库：XGBoost和LightGBM。
     16.4 
17、分析特征并进行分类器选择的部分过程
    1.稀疏的大规模数据:SGD和逻辑回归。
	2.归一化的实数数据:SVM
	3.预处理做的不好的数据，弱分类器集成。
	4.明显的概率分布问题:贝叶斯(用的少，但在有大语料库的文本处理问题中比较有效)。
	5.神经网络relu, relu, relu
	
18、随机森林（分类）
    from sklearn.ensemble import RandomForest Classifier
	data=[[0,0,0],[1,1,1],[2,2,2],[1,1,1],[2,2,2],[3,3,3],[1,1,1],[4,4,4]]
	target=[0,1,2,1,2,3,1,4]
	rf = RandomForestClassifier()
	rf.fit(data,target)
	print rf.predict_proba([[1,1,1]])
	
	随机森林（回归）
	from sklearn.ensemble import RandomForestRegressor
	data=[[0,0,0],[1,1,1],[2,2,2],[1,1,1],[2,2,2],[0,0,0]]
	target=[0,1,2,1,2,0]
	rf = RandomForestRegressor()
	rf.fit(data, target)
	print rf.predict([[1,1,1]])
	
19、1.如果把用户id看成一个类别特征，那么它可以取的值的数量就等于用户数。把这个用户id特
	 征与其他特征做笛卡尔积，就能产生庞大的特征集。做广告算法的公司经常宣称自己模型里
	 有几十上百亿的特征，基本都是这么搞出来的。
    
    2. 目前也已经有很多用于降维的方法。比如聚类、PCA等都是常用的降维方法1。
       但这类方法在特征量和样本量很多的时候本身就计算量很大，所以对大问题也基本无能为力。
	 
	3. 有一种很简单的降维方法——特征哈希（Feature Hashing）法：
	  特征哈希法的目标是把原始的高维特征向量压缩成较低维特征向量，且尽量不损失原始特征的
	  表达能力。参考：https://breezedeus.github.io/2014/11/20/breezedeus-feature-hashing.html
	
	4. 

20、分类的评价指标  【参考http://blog.csdn.net/u012089317/article/details/52156514】
    1、精准率和召回率
	    正确率：对于给定的测试数据集，分类器正确分类的样本数与总样本数之比
		精准率是针对预测结果而言的，它表示预测为正的样本中有多少是对的。
		召回率是针对原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。
		在信息检索领域，精确率和召回率又被称为查准率和查全率
	
	2、ROC曲线【讲的透彻：http://www.cnblogs.com/dlml/p/4403482.html】
	   引入 ROC ，ROC 曲线可以用于评价一个分类器好坏。
	   有两个指标 
	      横轴：假正类率(false postive rate FPR)特异度【代表分类器预测的正类中实际负实例占所有负实例的比例】 
	      纵轴：真正类率(true postive rate TPR)灵敏度【代表分类器预测的正类中实际正实例占所有正实例的比例】
	   反映了正类覆盖程度
	   故ROC曲线越靠拢(0,1)点，越偏离45度对角线越好
	
	3、AUC
	   AUC（Area Under Curve）被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。
	   AUC值越大的分类器，正确率越高
			AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，
			不存在完美分类器。
			0.5<AUC<1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
			AUC=0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
			AUC<0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测，因此不存在
			
	   
	问：既然已经这么多标准，为什么还要使用ROC和AUC呢？
	答：因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变换的时候，ROC曲线能够保持不变。在实际的数据集
	    中经常会出现样本类不平衡，即正负样本比例差距较大，而且测试数据中的正负样本也可能随着时间变化。

21、回归的评价指标
	1、平均绝对误差
		平均绝对误差MAE（Mean Absolute Error）又被称为L1范的损失
	2、平均平方误差
	   平均平方误差MSE（Mean Squared Error）又被称为L2范的损失

22、问：给你一个有1000列和1百万行的训练数据集，这个数据集是基于分类问题的。
	    经理要求你来降低该数据集的维度以减少模型计算时间，但你的机器内存有限。你会怎么做？	
    
	答：1、我们可以随机采样数据集。这意味着，我们可以创建一个较小的数据集，比如有1000个变量和30万行
		2、为了降低维度，我们可以把数值变量和分类变量分开，同时删掉相关联的变量。对于数值变量，
		   我们将使用相关性分析；对于分类变量，我们可以用卡方检验。
		3、另外，我们还可以使用PCA（主成分分析），并挑选可以解释在数据集中有最大偏差的成分。
		4、利用在线学习算法，如VowpalWabbit（在Python中可用）是一个不错的选择。
		5、利用Stochastic GradientDescent（随机梯度下降法）建立线性模型也很有帮助。
		6、我们也可以用我们对业务的理解来估计各预测变量对响应变量的影响的大小。但是，这是一个主观的方法，
		   如果没有找出有用的预测变量可能会导致信息的显著丢失。
23、问:协方差和相关性有什么区别？
    答：相关性是协方差的标准化格式。协方差本身很难做比较。为了解决这个问题，
	      我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。
		  
24、Gradient boosting算法（GBM）和随机森林都是基于树的算法，它们有什么区别？
    答：最根本的区别是，随机森林算法使用bagging技术做出预测；而GBM是采用boosting技术做预测的。
	  在bagging技术中，数据集用随机采样的方法被划分成n个样本。然后，使用单一的学习算法，在所有样本上建模。
		接着利用投票或者求平均来组合所得到的预测。
	  bagging是平行进行的，而boosting是在第一轮的预测之后，算法将分类出错的预测加高权重，使得它们可以在
		后续一轮中得到校正。这种给予分类出错的预测高权重的顺序过程持续进行，一直到达到停止标准为止。
		随机森林通过减少方差（主要方式）提高模型的精度。生成树之间是不相关的，以把方差的减少最大化。
		在另一方面，GBM提高了精度，同时减少了模型的偏差和方差。
25、问：你认为把分类变量当成连续型变量会更得到一个更好的预测模型吗？
    答：为了得到更好的预测，只有在分类变量在本质上是有序的情况下才可以被当做连续型变量来处理。

26、问：在k-means或kNN，我们是用欧氏距离来计算最近的邻居之间的距离。为什么不用曼哈顿距离？
	答：我们不用曼哈顿距离，因为它只计算水平或垂直距离，有维度的限制。另一方面，
	   欧氏距离可用于任何空间的距离计算问题。因为，数据点可以存在于任何空间，欧氏距离是更可行的选择。
	
	Eg：想象一下国际象棋棋盘，象或车所做的移动是由曼哈顿距离计算的，因为它们是在各自的水平和垂直方向做的运动。
	
27、为什么朴素贝叶斯如此“朴素”？
    答：因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，
	    这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。
28、问：花了几个小时后，现在你急于建一个高精度的模型。结果，你建了5 个GBM，没有一个模型比基准模型表现得更好
        最后，你决定将这些模型结合到一起，但是效果任然不好，原因可能在那里？
    答：组合的学习模型是基于合并弱的学习模型来创造一个强大的学习模型的想法，但是，只有当各模型之间没有相关性
	   的时候组合起来后才比较强大。由于我们已经试了5个GBM也没有提高精度，表明这些模型是相关的。具有相关性的模型
	   的问题是，所有的模型提供相同的信息。
29、海量数据处理【参考：http://blog.csdn.net/v_july_v/article/details/7382693】
    所谓海量数据处理，无非就是基于海量数据上的存储、处理、操作。
	何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。	
	
30、一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词，请给出思想，给出时间复杂度分析?

    方案1：如果文件比较大，无法一次性读入内存，可以采用hash取模的方法，将大文件分解为多个
	       小文件，对于单个小文件利用hash_map统计出每个小文件中10个最常出现的词，然后再进行归
	       并处理，找出最终的10个最常出现的词。
	方案2：通过hash取模将大文件分解为多个小文件后，除了可以用hash_map统计出每个小文件中10个最常出现的词，
		  也可以用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），最终同样找出出
		  现最频繁的前10个词（可用堆来实现），时间复杂度是O(n*lg10)。

31、问：一个文本文件，找出前10个经常出现的词，但这次文件比较长，说是上亿行或十亿行，总之无法一次读入内存，
          问最优解。？
    答：首先对文件hash并求模，将文件分解为多个小文件，对于单个文件利用上题的方法求出每个文件件中10个
	    最常出现的词。然后再进行归并处理，找出最终的10个最常出现的词。
		
		Eg: 如果一个大文件有十亿行，内存没办法一次性读取。file =[a,b,a,c,a,d,b,d,a,c,d..],以file文件为例
		    我们读取是不要一次性读完(readline,一行一行的读)，然后对每一个元素进行hash(obj)求模，将求模后
			相等的元素写入一个文件，然后关闭。照此方法，将这个大文件写到n个小文件里。
			
			对每个小文件都求出频率出现最高topN,然后将所有文件的最后结果进行归并，求出TOPN，即为这个大文件的TopN

32、问：现在有十个节点的服务器，分别有一个文件，要统计出这十个节点中所有文件的词出现频率最高的100个词？
    解决思路：分别对每一个节点求出Top100,这是map阶段
	          然后将各个节点的Top100，进行归并，求出总的Top100，这是Reduce阶段。
			  
		    

33、python的hash
		hash() 用于获取取一个对象（字符串或者数值等）的哈希值。
		  hash 语法：
		    hash(object)     # object -- 对象，对象可以是字符换、数字、集合、字典
			
	1. 返回对象的哈希值，用整数表示。哈希值在字典查找时，可用于快速比较键的值。
	     hash('good good study')  
			1032709256
	2、相等的数值，即使类型不一致，计算的哈希值是一样的。
			>>>hash('test')               # 字符串
				2314058222102390712
			>>> hash(1)                   # 数字
				1
			>>> hash(str([1,2,3]))         # 集合
				1335416675971793195
			>>> hash(str(sorted({'1':1}))) # 字典
				7666464346782421378

34. 100w个数中找出最大的100个数。
    方案1：采用局部淘汰法。选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中
		最小的元素比，如果比这个最小的要大，那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。
		依次循环，知道扫描了所有的元素。复杂度为O(100w*100)。
	方案2：采用快速排序的思想，每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用
		传统排序算法排序，取前100个。复杂度为O(100w*100)。
   方案3：在前面的题中，我们已经提到了，用一个含100个元素的最小堆完成。复杂度为O(100w*lg100)。
		接下来，咱们来看第二种方法，双层捅划分。


密匙二、多层划分
		多层划分----其实本质上还是分而治之的思想，重在“分”的技巧上！
	　　适用范围：第k大，中位数，不重复或重复的数字
	　　基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个
	    可以接受的范围内进行。

	
	
现在模型不足之处：
	1、每个列表可能不是完整的词表。那么当出现新的特征类别时，就无法对该条数据进行特征编码
		Eg：现在APP类共有1000类，但是突然出现一个新的APP名字，那么如果不将该APP名字加到该
		词表中去，就会对新的数据无法编码。
	2、
	
	
35、.Python中的模块和包是什么？
	在Python中，模块是搭建程序的一种方式。每一个Python代码文件都是一个模块，并可以引用其他的
	模块，比如对象和属性。
	
	一个包含许多Python代码的文件夹是一个包。一个包可以包含模块和子文件夹。
	
36、zip -r bao.zip filejia     将filejia打包成bao.zip

37、查看当前有那些登录账户
	w   或 who  或 users
38、蹬掉当前登录的某个账户
	CentOS踢出已登录用户的方法
	# pkill -KILL -t pts/0 (pts/0为w指令看到的用户终端号)
	
	[root@so3334-instance-7eewa27bv7jb home]# who
		root     tty1         2017-09-14 12:55
		ZPJ      pts/0        2017-09-14 16:17 (106.75.23.55)
		root     pts/1        2017-09-14 17:46 (106.75.23.55)
	
	Eg:想蹬掉第一个root用户：
	[root@so3334-instance-7eewa27bv7jb home]# pkill -KILL -t tty1

39、交叉熵(cross_entroy)
	0、主要用于度量两个概率分布间的差异性信息。语言模型的性能通常用交叉熵和复杂度（perplexity）
	   来衡量。
	1、交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的
		预测标记分布，交叉熵损失函数可以衡量p与q的相似性
	2、交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习
		速率降低的问题，因为学习速率可以被输出的误差所控制。
	3、在特征工程中，可以用来衡量两个随机变量之间的相似度
	4、在语言模型中（NLP）中，由于真实的分布p是未知的，模型是通过训练集得到的，交叉熵就是衡量
	   这个模型在测试集上的正确率
	   
	   
40、离散特征的编码分为两种情况：
     参考：http://blog.csdn.net/lujiandong1/article/details/52836051
	1、离散特征的取值之间没有大小的意义，比如color：[red,blue],那么就使用one-hot编码
	2、离散特征的取值有大小的意义，比如size:[X,XL,XXL],那么就使用数值的映射{X:1,XL:2,XXL:3}
	
41、KNN  && kmeans的区别   < http://www.cnblogs.com/190260995xixi/p/5945652.html >

	     K-Means                  |          KNN是分类算法

	目的是为了将一系列点集分成k类 |  目的是为了确定一个点的分类

	K-Means是聚类算法             |    KNN是分类算法

	监督学习，分类目标事先已知    |   非监督学习，将相似数据归到一
								  |   起从而得到分类，没有外部分类
								  
	训练数据集无label，是杂乱无章 |   训练数据集有label，已经是完全正确的数据
	的，经过聚类后才变得有点顺序，|
	先无序，后有序				  |


	有明显的前期训练过程          |  没有明显的前期训练过程，属于memory-based learning

	K值确定后每次结果可能不同，   |
	从 n个数据对象任意选择 k 个	  |   K值确定后每次结果固定
	对象作为初始聚类中心，随机    |
	性对结果影响较大	          |

	时间复杂度：O（n）            |   时间复杂度：O(n*k*t)，t为迭代次数

	相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法，一般用KD树来实现NN。

42、将原来的列表打乱
   from sklearn.utils import shuffle
   X = shuffle(X)
   X
   
43、RBF核在数据集不太充足的情况下有很好的结果，但是当数据量很大是就不太明显，
    而且运行速度非常非常非常的慢！ 所以我推荐使用线性核，运算速度快，而且效果比线性判别稍好一些
