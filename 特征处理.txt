***** 很全面的特征处理博客：http://www.cnblogs.com/jasonfreak/p/5448385.html

1、sklearn cross_val_score中的参数pre_dispatch
    ：http://sofasofa.io/forum_main_post.php?postid=1000493&
	
2、sklearn 中的模型对于大数据集如何处理?
	：https://www.zhihu.com/question/38034287
	
   平均数编码：针对高基数定性特征（类别特征）的数据预处理/特征工程 <业务上>
   https://zhuanlan.zhihu.com/p/26308272

   特征工程简介
   https://www.zhihu.com/people/ecore-sec/activities

   
3、对于大数据进行训练，内存会爆掉，方法有二：
    法一：使用增量学习的方法，
	     难点是写出代码比较费时间，另外就是能够使用的分类器不是很多
	    
	
	法二：分别对某一个特征进行ont-hot编码，然后，使用稀疏矩阵的存储的方法存储，将原先的内存释放掉，
	      以此重复，知道将所有的特征都用稀疏矩阵的存储方法进行储存了为止
		  
		 原始特征――――>one-hot编码―――>稀疏矩阵存储―――――>释放掉one-hot编码内存
		   
		  难点：假如数据量别大，可能没有办法存储一个特征的存储

		  
4、如何通过词典构造出one-hot编码？
  4.1 整体的总结
    总结： 有三种情况
	Case1、对类别的特征（eg:天气特征、名字等）进行one-hot编码，有两种方法
      法1、用sklearn函数，但要经过两步：
         1.1、LabelEncoder创建标签的整数编码， 
         1.2、OneHotEncoder用于创建整数编码值的one hot编码。        
      法2、如果数据量不大，可用pandas里面get_dummie函数直接进行处理
        
    
    Case2、对与整数值（如果整数只是标签的占位符）进行one hot编码，也有两种方法：
        法1、直接用OneHotEncoder()函数对其进行one hot编码
        法2、直接使用keras里面的to_categorical()函数对其进行one-hot编码
     
    Case:3：假如Case1遇到的数据特别大，每一列(特征)都有很多类别（上千），机器的内存没办法存储整个数据
           one-hot编码后的数据，这是就进行进行增量学习。对每一块数据进行one-hot编码，但值得注意的是，
             每一块数据必须整个数据集的字典下进行编码，相同这样特征维度才
         # http://182.150.37.21:8889/notebooks/Eguan/partial-fit/dict_ont-hot_turn.ipynb
        
    注意：这里进行one-hot编码时，默认情况是训练集和测试时一起进行编码的；否则在进行某些机器学习任务时，就会造成
          测试集和训练集特征维度不同的情况，进而造成错误
        
  4.2 对每一种方法进行分别解释   
		通常情况下，如果遇到定性特征/定量特征，共有4种方法对其进行one-hot编码：
		编码示例# http://182.150.37.21:8889/notebooks/Eguan/One-hot-encode/one-hot-encode.ipynb
	
	第1种：借助sklearn包,我们将使用scikit学习库的编码器。具体来说，包括以下两步：
		步1 将特征进行整数编码
		    LabelEncoder创建标签的整数编码，
		步2 将整数编码进行one-hot编码、
			OneHotEncoder用于创建整数编码值的one hot编码
		eg:
			from numpy import array
			from numpy import argmax
			from sklearn.preprocessing import LabelEncoder
			from sklearn.preprocessing import OneHotEncoder
			
			# define example
			data = ['cold', 'cold', 'warm', 'cold', 'hot', 'hot', 'warm', 'cold', 'warm', 'hot']
				# 将data想像成一个数据集中的天气特征列
			values = array(data)
			print 'Values值：',values

			# integer encode (整数编码)  
			label_encoder = LabelEncoder()
			integer_encoded = label_encoder.fit_transform(values)
			print '整数编码：',integer_encoded

			# binary encode （one-hot编码）
			onehot_encoder = OneHotEncoder(sparse=False)  
				#OneHotEncoder类默认将返回更高效的稀疏编码，这可能不适用于某些应用程序。
				#   例如使用Keras深度学习库。在这种情况下，我们通过设置sparse = #     
				   # False这个参数来禁用稀疏返回类型。
			integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
			onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
			print(onehot_encoded)
			
			
	第2种：可借pandas里面的get_dummies()函数
	    import pandas as pd
		df = pd.read_csv('filename')
	    df_ = pd.get_dummies(df)  
		
 
	第3种：借助keras里的to_categorical函数
	     # 这个和sklearn里面的OneHotEncoder类似，都智能对id（数字）进行编码。
		  # 可以先用sklearn的LabelEncoder函数将定性特征进行转化，然后在进行编码
		
	
	    from keras.utils import to_categorical
		
		# 例子 ，该函数只对1、3、6等这样整数id编码
		data = [1, 3, 2, 0, 3, 2, 2, 1, 0, 1]        # 将data想像成一个数据集中的某一列特征
		data = array(data)
		print(data)

		# one hot encode
		encoded = to_categorical(data)  # 只能对整型进行编码，将整数编码为二进制向量并打印。
		print(encoded)
		
	  另外，当keras对特征进行embedding(低维稠密的向量)，embedding的对象也是整型(ID),
	  所以，也需要先进行对文本(或定性特征)进行转化成整型对象
	
	第4种：手写one-hot编码
	    与前三种方法的区别在于：
			1、该方法可对整个数据集中的某一条数据进行编码
			2、进行增量计算时，对某一批次的数据进行one-hot编码
			
		步1、获取某一列定性值的词典
		步2、整数编码
		步3、one-hot编码
		
		# 如何从头开始自己的one hot编码
		
	Eg1：代码示例
		from numpy import argmax
		# 定义输入字符串
		data = 'hello world'
		print(data)

		# 步1 定义字典
		# 定义某一列字典（这一步可以python的set()函数实现）
		alphabet = 'abcdefghijklmnopqrstuvwxyz '
		# 定义字符到整数的映射（对字典个各元素进行映射）
		char_to_int = dict((c, i) for i, c in enumerate(alphabet))

		# 步2
		# 整数编码输入数据，即自然编码
		integer_encoded = [char_to_int[char] for char in data]
		print(integer_encoded) 
	
		# 步3
		# one-hot 编码
		onehot_encoded = list()
		for value in integer_encoded:
			   letter = [0 for _ in range(len(alphabet))]
			   letter[value] = 1
			   onehot_encoded.append(letter)
		print(onehot_encoded)
			   
	Eg2: 和上面代码类似，但是可对某一列的单个元素分别进行one-hot编码
	     http://182.150.37.21:8889/notebooks/Eguan/partial-fit/dict_ont-hot_turn.ipynb
	   #步1： 定义字典
		def generate_dict(A):
		B = set(A)
		length = len(B)
		dict = {}
		for index,item in enumerate(B):
			dict[item]=index
		return dict,length
	   
	   # 步2：对某一列数据进行one-hot编码
	    dicts,length = generate_dict(A)
		
	    import numpy as np
	    def turn_ont_hot(S,dicts,length):
			one_hot = [0 for i in xrange(length)]
			print '列表长度：',len(one_hot)
			for i in S:
				if i in dicts.keys():
					index  =dicts[i]
					one_hot[index] = 1
			return one_hot
			
		# 例子
		A = ['a','b','c','d','e','c','d','f','g','j','k']
		C = ['b','d','f']
		turn_ont_hot(C,dicts,length)
		
		
	# 步3 对某一列数据的每一个数据进行one-hot编码
	    # 应为每一列数据(加入n个)，代表是n条数据某一个特征的集合，我们
		  # 要做的是将每一条数据的该特征进行one-hot编码
		  
		# 测试，将某一列的每一个元素都转化为一个one-hot编码
		def turn_per_one_hot(X,dicts,length):
			result_c = []
			for item in X:
				result1 = turn_ont_hot(item,dicts,length)  #这里就看你要每个元素进行one-hot还是对整体进行one-hot编码了
				result_c.append(result1)
			return result_c
	
	
 ###################一整套特征处理的过程########################
 #####################sklearn 特征处理#########################
 ###################PCA 特征降维#############################
   1、参考：http://www.aboutyun.com/thread-21655-1-1.html
   2、 skelarn pca参数 ：http://doc.okbase.net/u012162613/archive/120946.html


4、sklearn 特征降维利器 ―― PCA & TSNE 
   <http://blog.csdn.net/lanchunhui/article/details/64923702?locationNum=11&fps=1>
  
   同为降维工具，二者的主要区别在于，所在的包不同（也即机制和原理不同）
        from sklearn.decomposition import PCA
        from sklearn.manifold import TSNE
		
    因为原理不同，导致，tsne 保留下的属性信息，更具代表性，也即最能体现样本间的差异；
    TSNE 运行极慢，PCA 则相对较快；

	因此更为一般的处理，尤其在展示（可视化）高维数据时，常常先用 PCA 进行降维，再使用 tsne：

	data_pca = PCA(n_components=50).fit_transform(data)
	data_pca_tsne = TSNE(n_components=2).fit_transform(data_pca)
	
	
5、特征降维
    对于分类来说，特征选择可以从众多的特征中选择对分类最重要的那些特征，去除原数据中的噪音。
    区别：        LDA与PCA的一大不同点在于，LDA是有监督的算法，而PCA是无监督的，因为PCA算法没有考虑数据的
	标签(类别)，只是把原数据映射到一些方差比较大的方向(基)上去而已。而LDA算法则考虑了数据的标签。
	
   1、有监督的特征选择(降维)、有标签 <LDA>
   
      使用方法：<使用lda库的LDA类选择特征的代码如下>
	    from sklearn.lda import LDA   
		#线性判别分析法，返回降维后的数据(也是特征数据)
		#参数n_components为降维后的维数
		X_trian = LDA(n_components=2).fit_transform(iris.data, iris.target)
		X_trian
		array([[-8.0849532 ,  0.32845422],
			   [-7.1471629 , -0.75547326],
			   [-7.51137789, -0.23807832],
			   [-6.83767561, -0.64288476],
					   ...
					   ...
			   [-8.15781367,  0.54063935],
			   [-7.72363087,  1.48232345])

			   
   2、无监督的特征降维<PCA>
	 使用方法：<使用decomposition库的PCA类选择特征的代码如下>
	    from sklearn.decomposition import PCA
 
	    #主成分分析法，返回降维后的数据(即特征数据)
	    #参数n_components为主成分数目
	    X_trian = PCA(n_components=2).fit_transform(iris.data)
		X_trian
   	        array([[-2.68420713,  0.32660731],
			      [-2.71539062, -0.16955685],
			      [-2.88981954, -0.13734561],
			      [-2.7464372 , -0.31112432],
				      ...
				      ...
			      [-2.72859298,  0.33392456],
			      [-2.27989736,  0.74778271])
	
	查看资料：
		1、pca不是很适合分类问题的降维
		2、谨慎使用非监督降维，比如PCA、topic model、PNF等，很可能的结果是你一降维，
		   啥信息都没有了，分类水平不如NB
	